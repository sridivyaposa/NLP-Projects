{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom nltk.corpus import twitter_samples\nimport nltk","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('twitter_samples')\ntwitter_samples.fileids()","execution_count":4,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n","name":"stdout"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare train and test data\nall_tweets = positive_tweets + negative_tweets\nall_sentiments = np.ones(5000).tolist() + np.zeros(5000).tolist()\nfrom sklearn.model_selection import train_test_split\ntrain_x,test_x,train_y, test_y = train_test_split(all_tweets,all_sentiments,random_state=101, test_size=0.2)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport re\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\ndef process_tweet(tweet):\n    \"\"\"\n    Generate tokens from the given tweet\n    input: \n        tweet: A string containing a tweet\n    output: Tokens after\n        clean_tokens: Tokens of the processed tweet\n    \"\"\"\n    stopwords_list = stopwords.words('english')\n    stemmer = PorterStemmer()\n    tweet = re.sub(r'RT\\s*','',tweet)\n    tweet = re.sub(r'\\$\\S*','',tweet)\n    tweet = re.sub(r'https?:\\/\\/\\S*','',tweet)\n    tweet = re.sub(r'#','',tweet)\n    tokenizer = TweetTokenizer(reduce_len=True,preserve_case=False, strip_handles=True)\n    tokens = tokenizer.tokenize(tweet)\n    clean_tokens = []\n    for word in tokens:\n        if (word not in stopwords_list) and (word not in string.punctuation):\n            clean_tokens.append(stemmer.stem(word))\n    return clean_tokens","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Frequency dictionary\ndef build_freqs(tweets,sentiments):\n    \"\"\"\n    Build positive and negative frequencies of each word in the corpus\n    Input:\n        tweets: A list of tweets\n        sentiments: a list of corresponding sentiments\n    Output:\n        freqs: A dictionary containing frequencies --> (word,sentiment):frequency\n    \"\"\"\n    freqs = {}\n    for tweet,sentiment in zip(tweets,sentiments):\n        for word in process_tweet(tweet):\n            freqs[(word,sentiment)] = freqs.get((word,sentiment),0)+1\n    return freqs\n            ","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate log(P(𝑊𝑝𝑜𝑠)/P(𝑊𝑛𝑒𝑔)) for each word, where\n$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V} $$$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V} $$\n\n$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_naive_bayes(freqs, train_x,train_y):\n    \"\"\"\n    train the model given freqs dict,tweets and respective sentiments \n    Input:\n    freqs: dictionary containing frequencies of words in corpus\n    train_x: tweets to be trained\n    train_y: labels of the tweets\n    Output:\n    loglikelihood: A dictionary containing log likelihoods of each word\n    logprior: ratio of positive tweets and negative tweets; removes bias if the dataset is unbalanced\n    \"\"\"\n    Npos = 0\n    Nneg = 0\n    for key in freqs.keys():\n        if key[1] == 1:\n            Npos += freqs.get(key)\n        else:\n            Nneg += freqs.get(key)\n    unique_words = set([key[0] for key in freqs.keys()])\n    V = len(unique_words)\n    Dpos = len([y for y in train_y if y==1])\n    Dneg = len(train_y) - Dpos\n    logprior = np.log(Dpos/Dneg)\n    loglikelihood = {}\n    for word in unique_words:\n        P_Wpos = (freqs.get((word,1),0) + 1)/(Npos+V)\n        P_Wneg = (freqs.get((word,0),0) + 1)/(Nneg+V)\n        loglikelihood[word] = np.log(P_Wpos/P_Wneg)\n    return logprior, loglikelihood","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freqs = build_freqs(train_x, train_y)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Calculate the probability of tweet being positive/negative by adding all the loglikelihoods of all words in the tweet. If Loglikelihood>0 then tweet is positive otherwise negative"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_naive_bayes(tweet,logprior,loglikelihood):\n    \"\"\"\n    predict loglikelihood of tweet being positive given trained loglikelihood and logprior\n    Input:\n    tweet: input tweet\n    logprior: trained logprior\n    loglikelihood: trained likelihood\n    Output:\n    p: logliklihood of tweet being positive\n    \"\"\"\n    p = logprior\n    for word in process_tweet(tweet):\n        p += loglikelihood.get(word,0)\n    return p","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_naive_bayes(test_x,test_y,logprior,loglikelihood):\n    \"\"\"\n    test the model\n    Input:\n    test_x: unseen tweets for validation\n    test_y: sentiments of tweets\n    logprior: trained logprior\n    loglikelihood: trained likelihood\n    Output:\n    accuracy: accuracy of the model on test set\n    \"\"\"\n    pred = []\n    for tweet in test_x:\n        p = predict_naive_bayes(tweet,logprior,loglikelihood)\n        if p > 0:\n            pred.append(1)\n        else:\n            pred.append(0)\n    accuracy = sum(np.squeeze(pred)==np.squeeze(test_y))/len(test_y)\n    accuracy = accuracy * 100\n    return accuracy","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy of the naive bayes model is {}%\".format(test_naive_bayes(test_x,test_y,logprior, loglikelihood)))","execution_count":27,"outputs":[{"output_type":"stream","text":"Accuracy of the naive bayes model is 99.7%\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}