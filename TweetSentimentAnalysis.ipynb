{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import twitter_samples","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Download Twitter samples\nnltk.download('twitter_samples')\nprint(\"Fields \",twitter_samples.fileids())","execution_count":42,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package twitter_samples to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\nFields  ['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get positive and negative tweets from respective json files\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at sample tweets\npositive_tweets[:5]","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n '@97sides CONGRATS :)',\n 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport string\nfrom nltk.tokenize import TweetTokenizer\n#Function for preprocessing the data and to get feature vectors\ndef process_tweet(tweet):\n    \"\"\"\n    Generate tokens from the given tweet\n    input: \n        tweet: A string containing a tweet\n    output: Tokens after\n        clean_tokens: Tokens of the processed tweet\n    \"\"\"\n    stop_words = stopwords.words('english')\n    stemmer = PorterStemmer()\n    #Remove hyperlinks, Retweet \"RT\" text, #, $stock tickers\n    tweet = re.sub(r'\\$\\w*','',tweet) # Removes words like $StockTicker #\\w --> word character\n    tweet = re.sub(r'RT\\s*','',tweet) # Remove RT text # \\s --> space character\n    tweet = re.sub(r'https?:\\/\\/\\S*','',tweet) # Removes links # ? --> 0 or 1 occurence of previous charcter #\\S any character except space characters (opposite of \\s)\n    tweet = re.sub(r'#','',tweet)\n    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n    tokens = tokenizer.tokenize(tweet)\n    clean_tokens = []\n    for word in tokens:\n        if (word not in stop_words) and (word not in string.punctuation):\n            clean_tokens.append(stemmer.stem(word))\n    return clean_tokens\n","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function for preprocessing the data and to get feature vectors\ndef build_frequencies(tweets, sentiments):\n    \"\"\"\n    Build positive and negative frequencies of each word in the corpus\n    Input:\n        tweets: A list of tweets\n        sentiments: a list of corresponding sentiments\n    Output:\n        freqs: A dictionary containing frequencies --> (word,sentiment):frequency\n    \"\"\"\n    freqs = {}\n    for tweet,sentiment in zip(tweets,sentiments):\n        for token in process_tweet(tweet):\n            freqs[(token,sentiment)] = freqs.get((token,sentiment),0) + 1\n        \n    return freqs","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combine positive and negative tweets and prepare train and test sets:\n#positive_tweets.extend(negative_tweets)\nall_tweets = positive_tweets + negative_tweets\nall_sentiments = np.ones(5000).tolist() + np.zeros(5000).tolist()\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(all_tweets,all_sentiments,test_size=0.2,random_state=101)\nprint(\"length of training tweets : \",len(X_train))\nprint(\"length of testing tweets : \",len(X_test))\nfor tweet,label in zip(X_train[:5],y_train[:5]):\n    print(\" {} {} \".format(tweet,label))","execution_count":47,"outputs":[{"output_type":"stream","text":"length of training tweets :  8000\nlength of testing tweets :  2000\n @Uber @Walls Why is there no icecream in Leeds? :( 0.0 \n Pengen boxing :( (at @golds_indonesia) — https://t.co/qXG4UNA4Fn 0.0 \n @vapemestoopid Ok,the first time we chat,and then i made such a joke lol .I believe you wont forget me,will u ? :) my name is @DannaQiu 1.0 \n @iFazy nhe Yar :( 0.0 \n ♛♛♛\n》》》》 \nI LOVE YOU SO MUCH.\nI BELİEVE THAT HE WİLL FOLLOW.\nPLEASE FOLLOW ME PLEASE JUSTİN @justinbieber :( x15.340\n》》》》ＳＥＥ ＭＥ\n♛♛♛ 0.0 \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"freqs = build_frequencies(X_train, y_train)","execution_count":48,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression:**\n$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n\nSigmoid\n$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$ \n\nCost Function\n$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) $$\n\nGradient of the Cost function:\n$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j $$\n\nUpdate the weights by subtracting fraction of derivative of loss function from weights:\n$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(x,y,theta,alpha, num_iters):\n    \"\"\"\n    Perform gradient descent of logistic regression\n    Input:\n        x: feature matrix(m,n+1), n+1 --> number of features including the bias\n        y: True label(m,1)\n        theta: Weights(n+1,1)\n        alpha: Learning rate\n        num_iters: Number of iteration you want to train your model for\n    Output:\n        J: Final loss after num_iters\n        theta: Adjusted weights after num_iters\n    \"\"\"\n    m = x.shape[0]\n    for i in range(num_iters):\n        z = np.dot(x,theta)\n        h = 1/(1+np.exp(-z))\n        J = -1./m * (np.dot(y.transpose(),np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))\n        theta = theta - (alpha/m) * np.dot(x.transpose(), (h - y))\n        print(\" Loss {} in iteration {}\".format(J, i))\n    J = float(J)\n    return J, theta\n    ","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features(tweet,freqs):\n    \"\"\"\n    Extract features for each tweet using the freqs dictionary\n    Input: \n    tweet: A string containing a tweet\n    freqs: Frequency dictionary containing frequencies for each word -->(word,label):freq\n    Output:\n    x: feature vector(1,3)\n    \"\"\"\n    x = np.zeros((1,3))\n    x[0,0] = 1 #bias\n    for word in process_tweet(tweet):\n        x[0,1] += freqs.get((word,1),0)\n        x[0,2] += freqs.get((word,0),0)\n    return x","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract features from each tweet and append to feature matrix X\nm = len(X_train)\nX = np.zeros((m,3))\nfor i in range(m):\n    X[i,:] = extract_features(X_train[i],freqs)\nY = np.array(y_train)\nY = Y.reshape(-1,1)\nprint(\"Y shape: \",Y.shape)\n#Gradient Descent\nJ, theta = gradient_descent(X,Y,np.zeros((3,1)),1e-9,1000)\nprint(\"Final training loss \",J)","execution_count":51,"outputs":[{"output_type":"stream","text":"Y shape:  (8000, 1)\n Loss [[0.69314718]] in iteration 0\n Loss [[0.6920177]] in iteration 1\n Loss [[0.6908917]] in iteration 2\n Loss [[0.68976917]] in iteration 3\n Loss [[0.68865009]] in iteration 4\n Loss [[0.68753445]] in iteration 5\n Loss [[0.68642224]] in iteration 6\n Loss [[0.68531345]] in iteration 7\n Loss [[0.68420806]] in iteration 8\n Loss [[0.68310608]] in iteration 9\n Loss [[0.68200747]] in iteration 10\n Loss [[0.68091224]] in iteration 11\n Loss [[0.67982037]] in iteration 12\n Loss [[0.67873184]] in iteration 13\n Loss [[0.67764666]] in iteration 14\n Loss [[0.6765648]] in iteration 15\n Loss [[0.67548625]] in iteration 16\n Loss [[0.67441101]] in iteration 17\n Loss [[0.67333906]] in iteration 18\n Loss [[0.67227039]] in iteration 19\n Loss [[0.67120498]] in iteration 20\n Loss [[0.67014284]] in iteration 21\n Loss [[0.66908393]] in iteration 22\n Loss [[0.66802826]] in iteration 23\n Loss [[0.66697582]] in iteration 24\n Loss [[0.66592658]] in iteration 25\n Loss [[0.66488054]] in iteration 26\n Loss [[0.6638377]] in iteration 27\n Loss [[0.66279802]] in iteration 28\n Loss [[0.66176151]] in iteration 29\n Loss [[0.66072816]] in iteration 30\n Loss [[0.65969794]] in iteration 31\n Loss [[0.65867086]] in iteration 32\n Loss [[0.6576469]] in iteration 33\n Loss [[0.65662604]] in iteration 34\n Loss [[0.65560828]] in iteration 35\n Loss [[0.6545936]] in iteration 36\n Loss [[0.653582]] in iteration 37\n Loss [[0.65257346]] in iteration 38\n Loss [[0.65156797]] in iteration 39\n Loss [[0.65056553]] in iteration 40\n Loss [[0.64956611]] in iteration 41\n Loss [[0.6485697]] in iteration 42\n Loss [[0.64757631]] in iteration 43\n Loss [[0.64658591]] in iteration 44\n Loss [[0.64559849]] in iteration 45\n Loss [[0.64461405]] in iteration 46\n Loss [[0.64363256]] in iteration 47\n Loss [[0.64265403]] in iteration 48\n Loss [[0.64167844]] in iteration 49\n Loss [[0.64070578]] in iteration 50\n Loss [[0.63973603]] in iteration 51\n Loss [[0.63876919]] in iteration 52\n Loss [[0.63780525]] in iteration 53\n Loss [[0.63684419]] in iteration 54\n Loss [[0.63588601]] in iteration 55\n Loss [[0.63493069]] in iteration 56\n Loss [[0.63397822]] in iteration 57\n Loss [[0.63302859]] in iteration 58\n Loss [[0.6320818]] in iteration 59\n Loss [[0.63113782]] in iteration 60\n Loss [[0.63019666]] in iteration 61\n Loss [[0.62925829]] in iteration 62\n Loss [[0.62832271]] in iteration 63\n Loss [[0.62738991]] in iteration 64\n Loss [[0.62645987]] in iteration 65\n Loss [[0.62553259]] in iteration 66\n Loss [[0.62460806]] in iteration 67\n Loss [[0.62368626]] in iteration 68\n Loss [[0.62276719]] in iteration 69\n Loss [[0.62185083]] in iteration 70\n Loss [[0.62093718]] in iteration 71\n Loss [[0.62002622]] in iteration 72\n Loss [[0.61911795]] in iteration 73\n Loss [[0.61821234]] in iteration 74\n Loss [[0.61730941]] in iteration 75\n Loss [[0.61640912]] in iteration 76\n Loss [[0.61551148]] in iteration 77\n Loss [[0.61461647]] in iteration 78\n Loss [[0.61372408]] in iteration 79\n Loss [[0.6128343]] in iteration 80\n Loss [[0.61194713]] in iteration 81\n Loss [[0.61106255]] in iteration 82\n Loss [[0.61018055]] in iteration 83\n Loss [[0.60930113]] in iteration 84\n Loss [[0.60842426]] in iteration 85\n Loss [[0.60754995]] in iteration 86\n Loss [[0.60667819]] in iteration 87\n Loss [[0.60580896]] in iteration 88\n Loss [[0.60494225]] in iteration 89\n Loss [[0.60407805]] in iteration 90\n Loss [[0.60321636]] in iteration 91\n Loss [[0.60235717]] in iteration 92\n Loss [[0.60150046]] in iteration 93\n Loss [[0.60064622]] in iteration 94\n Loss [[0.59979445]] in iteration 95\n Loss [[0.59894514]] in iteration 96\n Loss [[0.59809827]] in iteration 97\n Loss [[0.59725385]] in iteration 98\n Loss [[0.59641185]] in iteration 99\n Loss [[0.59557227]] in iteration 100\n Loss [[0.5947351]] in iteration 101\n Loss [[0.59390033]] in iteration 102\n Loss [[0.59306795]] in iteration 103\n Loss [[0.59223796]] in iteration 104\n Loss [[0.59141034]] in iteration 105\n Loss [[0.59058508]] in iteration 106\n Loss [[0.58976218]] in iteration 107\n Loss [[0.58894162]] in iteration 108\n Loss [[0.5881234]] in iteration 109\n Loss [[0.58730751]] in iteration 110\n Loss [[0.58649394]] in iteration 111\n Loss [[0.58568267]] in iteration 112\n Loss [[0.58487371]] in iteration 113\n Loss [[0.58406704]] in iteration 114\n Loss [[0.58326266]] in iteration 115\n Loss [[0.58246055]] in iteration 116\n Loss [[0.58166071]] in iteration 117\n Loss [[0.58086312]] in iteration 118\n Loss [[0.58006779]] in iteration 119\n Loss [[0.5792747]] in iteration 120\n Loss [[0.57848383]] in iteration 121\n Loss [[0.57769519]] in iteration 122\n Loss [[0.57690877]] in iteration 123\n Loss [[0.57612455]] in iteration 124\n Loss [[0.57534254]] in iteration 125\n Loss [[0.57456271]] in iteration 126\n Loss [[0.57378506]] in iteration 127\n Loss [[0.57300959]] in iteration 128\n Loss [[0.57223628]] in iteration 129\n Loss [[0.57146513]] in iteration 130\n Loss [[0.57069613]] in iteration 131\n Loss [[0.56992927]] in iteration 132\n Loss [[0.56916454]] in iteration 133\n Loss [[0.56840193]] in iteration 134\n Loss [[0.56764144]] in iteration 135\n Loss [[0.56688306]] in iteration 136\n Loss [[0.56612679]] in iteration 137\n Loss [[0.5653726]] in iteration 138\n Loss [[0.5646205]] in iteration 139\n Loss [[0.56387047]] in iteration 140\n Loss [[0.56312252]] in iteration 141\n Loss [[0.56237662]] in iteration 142\n Loss [[0.56163278]] in iteration 143\n Loss [[0.56089098]] in iteration 144\n Loss [[0.56015123]] in iteration 145\n Loss [[0.5594135]] in iteration 146\n Loss [[0.5586778]] in iteration 147\n Loss [[0.55794411]] in iteration 148\n Loss [[0.55721242]] in iteration 149\n Loss [[0.55648274]] in iteration 150\n Loss [[0.55575505]] in iteration 151\n Loss [[0.55502935]] in iteration 152\n Loss [[0.55430563]] in iteration 153\n Loss [[0.55358387]] in iteration 154\n Loss [[0.55286408]] in iteration 155\n Loss [[0.55214624]] in iteration 156\n Loss [[0.55143035]] in iteration 157\n Loss [[0.5507164]] in iteration 158\n Loss [[0.55000439]] in iteration 159\n Loss [[0.54929431]] in iteration 160\n Loss [[0.54858614]] in iteration 161\n Loss [[0.54787988]] in iteration 162\n Loss [[0.54717553]] in iteration 163\n Loss [[0.54647308]] in iteration 164\n Loss [[0.54577252]] in iteration 165\n Loss [[0.54507385]] in iteration 166\n Loss [[0.54437705]] in iteration 167\n Loss [[0.54368212]] in iteration 168\n Loss [[0.54298906]] in iteration 169\n Loss [[0.54229785]] in iteration 170\n Loss [[0.54160849]] in iteration 171\n Loss [[0.54092098]] in iteration 172\n Loss [[0.5402353]] in iteration 173\n Loss [[0.53955145]] in iteration 174\n Loss [[0.53886943]] in iteration 175\n Loss [[0.53818921]] in iteration 176\n Loss [[0.53751081]] in iteration 177\n Loss [[0.53683421]] in iteration 178\n Loss [[0.53615941]] in iteration 179\n Loss [[0.5354864]] in iteration 180\n Loss [[0.53481517]] in iteration 181\n Loss [[0.53414571]] in iteration 182\n Loss [[0.53347803]] in iteration 183\n Loss [[0.53281211]] in iteration 184\n Loss [[0.53214795]] in iteration 185\n Loss [[0.53148554]] in iteration 186\n Loss [[0.53082487]] in iteration 187\n Loss [[0.53016594]] in iteration 188\n Loss [[0.52950874]] in iteration 189\n Loss [[0.52885326]] in iteration 190\n Loss [[0.52819951]] in iteration 191\n Loss [[0.52754747]] in iteration 192\n Loss [[0.52689713]] in iteration 193\n Loss [[0.5262485]] in iteration 194\n Loss [[0.52560156]] in iteration 195\n Loss [[0.52495631]] in iteration 196\n Loss [[0.52431274]] in iteration 197\n Loss [[0.52367084]] in iteration 198\n Loss [[0.52303062]] in iteration 199\n Loss [[0.52239206]] in iteration 200\n Loss [[0.52175516]] in iteration 201\n Loss [[0.52111991]] in iteration 202\n Loss [[0.52048631]] in iteration 203\n Loss [[0.51985434]] in iteration 204\n Loss [[0.51922401]] in iteration 205\n Loss [[0.51859531]] in iteration 206\n Loss [[0.51796823]] in iteration 207\n Loss [[0.51734277]] in iteration 208\n Loss [[0.51671892]] in iteration 209\n Loss [[0.51609668]] in iteration 210\n Loss [[0.51547603]] in iteration 211\n Loss [[0.51485698]] in iteration 212\n Loss [[0.51423952]] in iteration 213\n Loss [[0.51362364]] in iteration 214\n Loss [[0.51300933]] in iteration 215\n Loss [[0.5123966]] in iteration 216\n Loss [[0.51178543]] in iteration 217\n Loss [[0.51117583]] in iteration 218\n Loss [[0.51056778]] in iteration 219\n Loss [[0.50996127]] in iteration 220\n Loss [[0.50935631]] in iteration 221\n Loss [[0.50875289]] in iteration 222\n Loss [[0.50815101]] in iteration 223\n Loss [[0.50755064]] in iteration 224\n Loss [[0.50695181]] in iteration 225\n Loss [[0.50635448]] in iteration 226\n Loss [[0.50575867]] in iteration 227\n Loss [[0.50516437]] in iteration 228\n Loss [[0.50457156]] in iteration 229\n Loss [[0.50398025]] in iteration 230\n Loss [[0.50339044]] in iteration 231\n Loss [[0.5028021]] in iteration 232\n Loss [[0.50221525]] in iteration 233\n Loss [[0.50162987]] in iteration 234\n Loss [[0.50104596]] in iteration 235\n Loss [[0.50046351]] in iteration 236\n Loss [[0.49988253]] in iteration 237\n Loss [[0.499303]] in iteration 238\n Loss [[0.49872491]] in iteration 239\n Loss [[0.49814827]] in iteration 240\n Loss [[0.49757307]] in iteration 241\n Loss [[0.49699931]] in iteration 242\n Loss [[0.49642697]] in iteration 243\n Loss [[0.49585606]] in iteration 244\n Loss [[0.49528656]] in iteration 245\n Loss [[0.49471848]] in iteration 246\n Loss [[0.49415181]] in iteration 247\n Loss [[0.49358655]] in iteration 248\n Loss [[0.49302268]] in iteration 249\n Loss [[0.49246021]] in iteration 250\n Loss [[0.49189913]] in iteration 251\n Loss [[0.49133943]] in iteration 252\n Loss [[0.49078111]] in iteration 253\n Loss [[0.49022417]] in iteration 254\n Loss [[0.4896686]] in iteration 255\n Loss [[0.4891144]] in iteration 256\n Loss [[0.48856156]] in iteration 257\n Loss [[0.48801007]] in iteration 258\n Loss [[0.48745994]] in iteration 259\n Loss [[0.48691116]] in iteration 260\n Loss [[0.48636371]] in iteration 261\n Loss [[0.48581761]] in iteration 262\n Loss [[0.48527284]] in iteration 263\n Loss [[0.4847294]] in iteration 264\n Loss [[0.48418729]] in iteration 265\n Loss [[0.48364649]] in iteration 266\n Loss [[0.48310701]] in iteration 267\n Loss [[0.48256884]] in iteration 268\n Loss [[0.48203198]] in iteration 269\n Loss [[0.48149642]] in iteration 270\n Loss [[0.48096216]] in iteration 271\n Loss [[0.4804292]] in iteration 272\n Loss [[0.47989752]] in iteration 273\n Loss [[0.47936713]] in iteration 274\n Loss [[0.47883801]] in iteration 275\n Loss [[0.47831018]] in iteration 276\n Loss [[0.47778362]] in iteration 277\n Loss [[0.47725832]] in iteration 278\n Loss [[0.47673429]] in iteration 279\n Loss [[0.47621151]] in iteration 280\n Loss [[0.47569]] in iteration 281\n Loss [[0.47516973]] in iteration 282\n Loss [[0.47465071]] in iteration 283\n Loss [[0.47413293]] in iteration 284\n Loss [[0.47361639]] in iteration 285\n Loss [[0.47310108]] in iteration 286\n Loss [[0.47258701]] in iteration 287\n Loss [[0.47207415]] in iteration 288\n Loss [[0.47156253]] in iteration 289\n Loss [[0.47105212]] in iteration 290\n Loss [[0.47054292]] in iteration 291\n Loss [[0.47003493]] in iteration 292\n Loss [[0.46952815]] in iteration 293\n Loss [[0.46902257]] in iteration 294\n Loss [[0.46851819]] in iteration 295\n Loss [[0.468015]] in iteration 296\n Loss [[0.467513]] in iteration 297\n Loss [[0.46701218]] in iteration 298\n Loss [[0.46651255]] in iteration 299\n Loss [[0.4660141]] in iteration 300\n Loss [[0.46551682]] in iteration 301\n Loss [[0.46502071]] in iteration 302\n Loss [[0.46452577]] in iteration 303\n Loss [[0.46403199]] in iteration 304\n Loss [[0.46353936]] in iteration 305\n Loss [[0.4630479]] in iteration 306\n Loss [[0.46255758]] in iteration 307\n Loss [[0.46206841]] in iteration 308\n Loss [[0.46158039]] in iteration 309\n Loss [[0.4610935]] in iteration 310\n Loss [[0.46060775]] in iteration 311\n Loss [[0.46012314]] in iteration 312\n Loss [[0.45963965]] in iteration 313\n","name":"stdout"},{"output_type":"stream","text":" Loss [[0.45915729]] in iteration 314\n Loss [[0.45867605]] in iteration 315\n Loss [[0.45819592]] in iteration 316\n Loss [[0.45771691]] in iteration 317\n Loss [[0.45723901]] in iteration 318\n Loss [[0.45676222]] in iteration 319\n Loss [[0.45628653]] in iteration 320\n Loss [[0.45581194]] in iteration 321\n Loss [[0.45533845]] in iteration 322\n Loss [[0.45486605]] in iteration 323\n Loss [[0.45439473]] in iteration 324\n Loss [[0.45392451]] in iteration 325\n Loss [[0.45345536]] in iteration 326\n Loss [[0.4529873]] in iteration 327\n Loss [[0.45252031]] in iteration 328\n Loss [[0.45205439]] in iteration 329\n Loss [[0.45158953]] in iteration 330\n Loss [[0.45112575]] in iteration 331\n Loss [[0.45066302]] in iteration 332\n Loss [[0.45020135]] in iteration 333\n Loss [[0.44974074]] in iteration 334\n Loss [[0.44928118]] in iteration 335\n Loss [[0.44882266]] in iteration 336\n Loss [[0.44836519]] in iteration 337\n Loss [[0.44790877]] in iteration 338\n Loss [[0.44745337]] in iteration 339\n Loss [[0.44699902]] in iteration 340\n Loss [[0.44654569]] in iteration 341\n Loss [[0.44609339]] in iteration 342\n Loss [[0.44564212]] in iteration 343\n Loss [[0.44519187]] in iteration 344\n Loss [[0.44474263]] in iteration 345\n Loss [[0.44429441]] in iteration 346\n Loss [[0.44384721]] in iteration 347\n Loss [[0.44340101]] in iteration 348\n Loss [[0.44295581]] in iteration 349\n Loss [[0.44251162]] in iteration 350\n Loss [[0.44206843]] in iteration 351\n Loss [[0.44162623]] in iteration 352\n Loss [[0.44118502]] in iteration 353\n Loss [[0.44074481]] in iteration 354\n Loss [[0.44030558]] in iteration 355\n Loss [[0.43986733]] in iteration 356\n Loss [[0.43943007]] in iteration 357\n Loss [[0.43899378]] in iteration 358\n Loss [[0.43855846]] in iteration 359\n Loss [[0.43812412]] in iteration 360\n Loss [[0.43769074]] in iteration 361\n Loss [[0.43725833]] in iteration 362\n Loss [[0.43682688]] in iteration 363\n Loss [[0.43639639]] in iteration 364\n Loss [[0.43596686]] in iteration 365\n Loss [[0.43553828]] in iteration 366\n Loss [[0.43511064]] in iteration 367\n Loss [[0.43468396]] in iteration 368\n Loss [[0.43425822]] in iteration 369\n Loss [[0.43383342]] in iteration 370\n Loss [[0.43340955]] in iteration 371\n Loss [[0.43298663]] in iteration 372\n Loss [[0.43256463]] in iteration 373\n Loss [[0.43214357]] in iteration 374\n Loss [[0.43172343]] in iteration 375\n Loss [[0.43130421]] in iteration 376\n Loss [[0.43088592]] in iteration 377\n Loss [[0.43046854]] in iteration 378\n Loss [[0.43005208]] in iteration 379\n Loss [[0.42963653]] in iteration 380\n Loss [[0.42922189]] in iteration 381\n Loss [[0.42880816]] in iteration 382\n Loss [[0.42839533]] in iteration 383\n Loss [[0.4279834]] in iteration 384\n Loss [[0.42757238]] in iteration 385\n Loss [[0.42716224]] in iteration 386\n Loss [[0.426753]] in iteration 387\n Loss [[0.42634465]] in iteration 388\n Loss [[0.42593719]] in iteration 389\n Loss [[0.42553061]] in iteration 390\n Loss [[0.42512491]] in iteration 391\n Loss [[0.42472009]] in iteration 392\n Loss [[0.42431615]] in iteration 393\n Loss [[0.42391309]] in iteration 394\n Loss [[0.42351089]] in iteration 395\n Loss [[0.42310956]] in iteration 396\n Loss [[0.4227091]] in iteration 397\n Loss [[0.4223095]] in iteration 398\n Loss [[0.42191077]] in iteration 399\n Loss [[0.42151289]] in iteration 400\n Loss [[0.42111587]] in iteration 401\n Loss [[0.42071969]] in iteration 402\n Loss [[0.42032437]] in iteration 403\n Loss [[0.4199299]] in iteration 404\n Loss [[0.41953628]] in iteration 405\n Loss [[0.41914349]] in iteration 406\n Loss [[0.41875155]] in iteration 407\n Loss [[0.41836044]] in iteration 408\n Loss [[0.41797017]] in iteration 409\n Loss [[0.41758073]] in iteration 410\n Loss [[0.41719212]] in iteration 411\n Loss [[0.41680434]] in iteration 412\n Loss [[0.41641738]] in iteration 413\n Loss [[0.41603125]] in iteration 414\n Loss [[0.41564593]] in iteration 415\n Loss [[0.41526144]] in iteration 416\n Loss [[0.41487776]] in iteration 417\n Loss [[0.41449489]] in iteration 418\n Loss [[0.41411283]] in iteration 419\n Loss [[0.41373158]] in iteration 420\n Loss [[0.41335114]] in iteration 421\n Loss [[0.4129715]] in iteration 422\n Loss [[0.41259266]] in iteration 423\n Loss [[0.41221461]] in iteration 424\n Loss [[0.41183737]] in iteration 425\n Loss [[0.41146092]] in iteration 426\n Loss [[0.41108525]] in iteration 427\n Loss [[0.41071038]] in iteration 428\n Loss [[0.4103363]] in iteration 429\n Loss [[0.409963]] in iteration 430\n Loss [[0.40959048]] in iteration 431\n Loss [[0.40921874]] in iteration 432\n Loss [[0.40884777]] in iteration 433\n Loss [[0.40847759]] in iteration 434\n Loss [[0.40810817]] in iteration 435\n Loss [[0.40773953]] in iteration 436\n Loss [[0.40737165]] in iteration 437\n Loss [[0.40700454]] in iteration 438\n Loss [[0.4066382]] in iteration 439\n Loss [[0.40627261]] in iteration 440\n Loss [[0.40590779]] in iteration 441\n Loss [[0.40554372]] in iteration 442\n Loss [[0.4051804]] in iteration 443\n Loss [[0.40481784]] in iteration 444\n Loss [[0.40445603]] in iteration 445\n Loss [[0.40409497]] in iteration 446\n Loss [[0.40373465]] in iteration 447\n Loss [[0.40337508]] in iteration 448\n Loss [[0.40301625]] in iteration 449\n Loss [[0.40265816]] in iteration 450\n Loss [[0.4023008]] in iteration 451\n Loss [[0.40194419]] in iteration 452\n Loss [[0.4015883]] in iteration 453\n Loss [[0.40123314]] in iteration 454\n Loss [[0.40087872]] in iteration 455\n Loss [[0.40052502]] in iteration 456\n Loss [[0.40017204]] in iteration 457\n Loss [[0.39981979]] in iteration 458\n Loss [[0.39946826]] in iteration 459\n Loss [[0.39911745]] in iteration 460\n Loss [[0.39876735]] in iteration 461\n Loss [[0.39841797]] in iteration 462\n Loss [[0.39806929]] in iteration 463\n Loss [[0.39772133]] in iteration 464\n Loss [[0.39737408]] in iteration 465\n Loss [[0.39702753]] in iteration 466\n Loss [[0.39668169]] in iteration 467\n Loss [[0.39633655]] in iteration 468\n Loss [[0.39599211]] in iteration 469\n Loss [[0.39564836]] in iteration 470\n Loss [[0.39530532]] in iteration 471\n Loss [[0.39496296]] in iteration 472\n Loss [[0.3946213]] in iteration 473\n Loss [[0.39428033]] in iteration 474\n Loss [[0.39394005]] in iteration 475\n Loss [[0.39360045]] in iteration 476\n Loss [[0.39326154]] in iteration 477\n Loss [[0.39292331]] in iteration 478\n Loss [[0.39258576]] in iteration 479\n Loss [[0.39224888]] in iteration 480\n Loss [[0.39191269]] in iteration 481\n Loss [[0.39157716]] in iteration 482\n Loss [[0.39124232]] in iteration 483\n Loss [[0.39090814]] in iteration 484\n Loss [[0.39057463]] in iteration 485\n Loss [[0.39024178]] in iteration 486\n Loss [[0.3899096]] in iteration 487\n Loss [[0.38957809]] in iteration 488\n Loss [[0.38924724]] in iteration 489\n Loss [[0.38891704]] in iteration 490\n Loss [[0.3885875]] in iteration 491\n Loss [[0.38825862]] in iteration 492\n Loss [[0.38793039]] in iteration 493\n Loss [[0.38760282]] in iteration 494\n Loss [[0.38727589]] in iteration 495\n Loss [[0.38694961]] in iteration 496\n Loss [[0.38662398]] in iteration 497\n Loss [[0.38629899]] in iteration 498\n Loss [[0.38597465]] in iteration 499\n Loss [[0.38565095]] in iteration 500\n Loss [[0.38532788]] in iteration 501\n Loss [[0.38500546]] in iteration 502\n Loss [[0.38468367]] in iteration 503\n Loss [[0.38436251]] in iteration 504\n Loss [[0.38404199]] in iteration 505\n Loss [[0.38372209]] in iteration 506\n Loss [[0.38340283]] in iteration 507\n Loss [[0.38308419]] in iteration 508\n Loss [[0.38276617]] in iteration 509\n Loss [[0.38244878]] in iteration 510\n Loss [[0.38213201]] in iteration 511\n Loss [[0.38181586]] in iteration 512\n Loss [[0.38150033]] in iteration 513\n Loss [[0.38118542]] in iteration 514\n Loss [[0.38087112]] in iteration 515\n Loss [[0.38055743]] in iteration 516\n Loss [[0.38024436]] in iteration 517\n Loss [[0.37993189]] in iteration 518\n Loss [[0.37962003]] in iteration 519\n Loss [[0.37930878]] in iteration 520\n Loss [[0.37899814]] in iteration 521\n Loss [[0.37868809]] in iteration 522\n Loss [[0.37837865]] in iteration 523\n Loss [[0.37806981]] in iteration 524\n Loss [[0.37776156]] in iteration 525\n Loss [[0.37745392]] in iteration 526\n Loss [[0.37714686]] in iteration 527\n Loss [[0.3768404]] in iteration 528\n Loss [[0.37653453]] in iteration 529\n Loss [[0.37622926]] in iteration 530\n Loss [[0.37592456]] in iteration 531\n Loss [[0.37562046]] in iteration 532\n Loss [[0.37531694]] in iteration 533\n Loss [[0.37501401]] in iteration 534\n Loss [[0.37471165]] in iteration 535\n Loss [[0.37440988]] in iteration 536\n Loss [[0.37410869]] in iteration 537\n Loss [[0.37380807]] in iteration 538\n Loss [[0.37350803]] in iteration 539\n Loss [[0.37320856]] in iteration 540\n Loss [[0.37290966]] in iteration 541\n Loss [[0.37261134]] in iteration 542\n Loss [[0.37231358]] in iteration 543\n Loss [[0.3720164]] in iteration 544\n Loss [[0.37171977]] in iteration 545\n Loss [[0.37142372]] in iteration 546\n Loss [[0.37112823]] in iteration 547\n Loss [[0.37083329]] in iteration 548\n Loss [[0.37053892]] in iteration 549\n Loss [[0.37024511]] in iteration 550\n Loss [[0.36995185]] in iteration 551\n Loss [[0.36965915]] in iteration 552\n Loss [[0.36936701]] in iteration 553\n Loss [[0.36907542]] in iteration 554\n Loss [[0.36878437]] in iteration 555\n Loss [[0.36849388]] in iteration 556\n Loss [[0.36820394]] in iteration 557\n Loss [[0.36791454]] in iteration 558\n Loss [[0.36762569]] in iteration 559\n Loss [[0.36733738]] in iteration 560\n Loss [[0.36704962]] in iteration 561\n Loss [[0.36676239]] in iteration 562\n Loss [[0.36647571]] in iteration 563\n Loss [[0.36618956]] in iteration 564\n Loss [[0.36590395]] in iteration 565\n Loss [[0.36561887]] in iteration 566\n Loss [[0.36533433]] in iteration 567\n Loss [[0.36505032]] in iteration 568\n Loss [[0.36476685]] in iteration 569\n Loss [[0.3644839]] in iteration 570\n Loss [[0.36420148]] in iteration 571\n Loss [[0.36391958]] in iteration 572\n Loss [[0.36363821]] in iteration 573\n Loss [[0.36335737]] in iteration 574\n Loss [[0.36307705]] in iteration 575\n Loss [[0.36279725]] in iteration 576\n Loss [[0.36251797]] in iteration 577\n Loss [[0.3622392]] in iteration 578\n Loss [[0.36196096]] in iteration 579\n Loss [[0.36168323]] in iteration 580\n Loss [[0.36140601]] in iteration 581\n Loss [[0.36112931]] in iteration 582\n Loss [[0.36085312]] in iteration 583\n Loss [[0.36057744]] in iteration 584\n Loss [[0.36030227]] in iteration 585\n Loss [[0.3600276]] in iteration 586\n Loss [[0.35975344]] in iteration 587\n Loss [[0.35947979]] in iteration 588\n Loss [[0.35920664]] in iteration 589\n Loss [[0.358934]] in iteration 590\n Loss [[0.35866185]] in iteration 591\n Loss [[0.3583902]] in iteration 592\n Loss [[0.35811906]] in iteration 593\n Loss [[0.35784841]] in iteration 594\n Loss [[0.35757825]] in iteration 595\n Loss [[0.35730859]] in iteration 596\n Loss [[0.35703942]] in iteration 597\n Loss [[0.35677075]] in iteration 598\n Loss [[0.35650256]] in iteration 599\n Loss [[0.35623487]] in iteration 600\n Loss [[0.35596766]] in iteration 601\n Loss [[0.35570094]] in iteration 602\n Loss [[0.3554347]] in iteration 603\n Loss [[0.35516895]] in iteration 604\n Loss [[0.35490369]] in iteration 605\n Loss [[0.3546389]] in iteration 606\n Loss [[0.3543746]] in iteration 607\n Loss [[0.35411077]] in iteration 608\n Loss [[0.35384742]] in iteration 609\n Loss [[0.35358455]] in iteration 610\n Loss [[0.35332216]] in iteration 611\n Loss [[0.35306023]] in iteration 612\n Loss [[0.35279879]] in iteration 613\n Loss [[0.35253781]] in iteration 614\n Loss [[0.35227731]] in iteration 615\n Loss [[0.35201727]] in iteration 616\n Loss [[0.3517577]] in iteration 617\n Loss [[0.3514986]] in iteration 618\n Loss [[0.35123997]] in iteration 619\n Loss [[0.3509818]] in iteration 620\n Loss [[0.35072409]] in iteration 621\n Loss [[0.35046685]] in iteration 622\n Loss [[0.35021007]] in iteration 623\n Loss [[0.34995375]] in iteration 624\n Loss [[0.34969788]] in iteration 625\n Loss [[0.34944248]] in iteration 626\n Loss [[0.34918753]] in iteration 627\n Loss [[0.34893303]] in iteration 628\n Loss [[0.348679]] in iteration 629\n Loss [[0.34842541]] in iteration 630\n Loss [[0.34817227]] in iteration 631\n Loss [[0.34791959]] in iteration 632\n Loss [[0.34766736]] in iteration 633\n Loss [[0.34741557]] in iteration 634\n Loss [[0.34716423]] in iteration 635\n Loss [[0.34691334]] in iteration 636\n Loss [[0.34666289]] in iteration 637\n Loss [[0.34641289]] in iteration 638\n Loss [[0.34616333]] in iteration 639\n Loss [[0.34591421]] in iteration 640\n Loss [[0.34566553]] in iteration 641\n Loss [[0.34541729]] in iteration 642\n Loss [[0.34516949]] in iteration 643\n Loss [[0.34492212]] in iteration 644\n Loss [[0.3446752]] in iteration 645\n Loss [[0.34442871]] in iteration 646\n Loss [[0.34418265]] in iteration 647\n Loss [[0.34393702]] in iteration 648\n Loss [[0.34369183]] in iteration 649\n Loss [[0.34344706]] in iteration 650\n Loss [[0.34320273]] in iteration 651\n Loss [[0.34295883]] in iteration 652\n Loss [[0.34271535]] in iteration 653\n Loss [[0.3424723]] in iteration 654\n","name":"stdout"},{"output_type":"stream","text":" Loss [[0.34222967]] in iteration 655\n Loss [[0.34198747]] in iteration 656\n Loss [[0.34174569]] in iteration 657\n Loss [[0.34150434]] in iteration 658\n Loss [[0.3412634]] in iteration 659\n Loss [[0.34102289]] in iteration 660\n Loss [[0.34078279]] in iteration 661\n Loss [[0.34054311]] in iteration 662\n Loss [[0.34030385]] in iteration 663\n Loss [[0.34006501]] in iteration 664\n Loss [[0.33982658]] in iteration 665\n Loss [[0.33958856]] in iteration 666\n Loss [[0.33935096]] in iteration 667\n Loss [[0.33911377]] in iteration 668\n Loss [[0.33887698]] in iteration 669\n Loss [[0.33864061]] in iteration 670\n Loss [[0.33840465]] in iteration 671\n Loss [[0.3381691]] in iteration 672\n Loss [[0.33793395]] in iteration 673\n Loss [[0.33769921]] in iteration 674\n Loss [[0.33746487]] in iteration 675\n Loss [[0.33723094]] in iteration 676\n Loss [[0.3369974]] in iteration 677\n Loss [[0.33676428]] in iteration 678\n Loss [[0.33653155]] in iteration 679\n Loss [[0.33629922]] in iteration 680\n Loss [[0.33606729]] in iteration 681\n Loss [[0.33583576]] in iteration 682\n Loss [[0.33560462]] in iteration 683\n Loss [[0.33537388]] in iteration 684\n Loss [[0.33514354]] in iteration 685\n Loss [[0.33491359]] in iteration 686\n Loss [[0.33468403]] in iteration 687\n Loss [[0.33445486]] in iteration 688\n Loss [[0.33422609]] in iteration 689\n Loss [[0.33399771]] in iteration 690\n Loss [[0.33376971]] in iteration 691\n Loss [[0.3335421]] in iteration 692\n Loss [[0.33331488]] in iteration 693\n Loss [[0.33308805]] in iteration 694\n Loss [[0.3328616]] in iteration 695\n Loss [[0.33263554]] in iteration 696\n Loss [[0.33240986]] in iteration 697\n Loss [[0.33218456]] in iteration 698\n Loss [[0.33195965]] in iteration 699\n Loss [[0.33173511]] in iteration 700\n Loss [[0.33151096]] in iteration 701\n Loss [[0.33128718]] in iteration 702\n Loss [[0.33106378]] in iteration 703\n Loss [[0.33084076]] in iteration 704\n Loss [[0.33061811]] in iteration 705\n Loss [[0.33039584]] in iteration 706\n Loss [[0.33017395]] in iteration 707\n Loss [[0.32995243]] in iteration 708\n Loss [[0.32973128]] in iteration 709\n Loss [[0.3295105]] in iteration 710\n Loss [[0.32929009]] in iteration 711\n Loss [[0.32907006]] in iteration 712\n Loss [[0.32885039]] in iteration 713\n Loss [[0.32863109]] in iteration 714\n Loss [[0.32841215]] in iteration 715\n Loss [[0.32819359]] in iteration 716\n Loss [[0.32797539]] in iteration 717\n Loss [[0.32775755]] in iteration 718\n Loss [[0.32754008]] in iteration 719\n Loss [[0.32732297]] in iteration 720\n Loss [[0.32710622]] in iteration 721\n Loss [[0.32688983]] in iteration 722\n Loss [[0.32667381]] in iteration 723\n Loss [[0.32645814]] in iteration 724\n Loss [[0.32624283]] in iteration 725\n Loss [[0.32602788]] in iteration 726\n Loss [[0.32581329]] in iteration 727\n Loss [[0.32559905]] in iteration 728\n Loss [[0.32538516]] in iteration 729\n Loss [[0.32517164]] in iteration 730\n Loss [[0.32495846]] in iteration 731\n Loss [[0.32474564]] in iteration 732\n Loss [[0.32453317]] in iteration 733\n Loss [[0.32432105]] in iteration 734\n Loss [[0.32410928]] in iteration 735\n Loss [[0.32389786]] in iteration 736\n Loss [[0.32368678]] in iteration 737\n Loss [[0.32347606]] in iteration 738\n Loss [[0.32326568]] in iteration 739\n Loss [[0.32305565]] in iteration 740\n Loss [[0.32284596]] in iteration 741\n Loss [[0.32263662]] in iteration 742\n Loss [[0.32242762]] in iteration 743\n Loss [[0.32221897]] in iteration 744\n Loss [[0.32201065]] in iteration 745\n Loss [[0.32180268]] in iteration 746\n Loss [[0.32159505]] in iteration 747\n Loss [[0.32138776]] in iteration 748\n Loss [[0.3211808]] in iteration 749\n Loss [[0.32097419]] in iteration 750\n Loss [[0.32076791]] in iteration 751\n Loss [[0.32056197]] in iteration 752\n Loss [[0.32035636]] in iteration 753\n Loss [[0.32015109]] in iteration 754\n Loss [[0.31994615]] in iteration 755\n Loss [[0.31974155]] in iteration 756\n Loss [[0.31953727]] in iteration 757\n Loss [[0.31933333]] in iteration 758\n Loss [[0.31912972]] in iteration 759\n Loss [[0.31892644]] in iteration 760\n Loss [[0.3187235]] in iteration 761\n Loss [[0.31852087]] in iteration 762\n Loss [[0.31831858]] in iteration 763\n Loss [[0.31811661]] in iteration 764\n Loss [[0.31791497]] in iteration 765\n Loss [[0.31771366]] in iteration 766\n Loss [[0.31751267]] in iteration 767\n Loss [[0.31731201]] in iteration 768\n Loss [[0.31711166]] in iteration 769\n Loss [[0.31691164]] in iteration 770\n Loss [[0.31671195]] in iteration 771\n Loss [[0.31651257]] in iteration 772\n Loss [[0.31631351]] in iteration 773\n Loss [[0.31611478]] in iteration 774\n Loss [[0.31591636]] in iteration 775\n Loss [[0.31571826]] in iteration 776\n Loss [[0.31552048]] in iteration 777\n Loss [[0.31532301]] in iteration 778\n Loss [[0.31512586]] in iteration 779\n Loss [[0.31492903]] in iteration 780\n Loss [[0.31473251]] in iteration 781\n Loss [[0.3145363]] in iteration 782\n Loss [[0.31434041]] in iteration 783\n Loss [[0.31414483]] in iteration 784\n Loss [[0.31394956]] in iteration 785\n Loss [[0.31375461]] in iteration 786\n Loss [[0.31355996]] in iteration 787\n Loss [[0.31336562]] in iteration 788\n Loss [[0.31317159]] in iteration 789\n Loss [[0.31297787]] in iteration 790\n Loss [[0.31278446]] in iteration 791\n Loss [[0.31259135]] in iteration 792\n Loss [[0.31239855]] in iteration 793\n Loss [[0.31220605]] in iteration 794\n Loss [[0.31201386]] in iteration 795\n Loss [[0.31182197]] in iteration 796\n Loss [[0.31163039]] in iteration 797\n Loss [[0.31143911]] in iteration 798\n Loss [[0.31124813]] in iteration 799\n Loss [[0.31105745]] in iteration 800\n Loss [[0.31086707]] in iteration 801\n Loss [[0.310677]] in iteration 802\n Loss [[0.31048722]] in iteration 803\n Loss [[0.31029774]] in iteration 804\n Loss [[0.31010855]] in iteration 805\n Loss [[0.30991967]] in iteration 806\n Loss [[0.30973108]] in iteration 807\n Loss [[0.30954279]] in iteration 808\n Loss [[0.30935479]] in iteration 809\n Loss [[0.30916708]] in iteration 810\n Loss [[0.30897967]] in iteration 811\n Loss [[0.30879256]] in iteration 812\n Loss [[0.30860573]] in iteration 813\n Loss [[0.3084192]] in iteration 814\n Loss [[0.30823296]] in iteration 815\n Loss [[0.30804701]] in iteration 816\n Loss [[0.30786135]] in iteration 817\n Loss [[0.30767597]] in iteration 818\n Loss [[0.30749089]] in iteration 819\n Loss [[0.3073061]] in iteration 820\n Loss [[0.30712159]] in iteration 821\n Loss [[0.30693737]] in iteration 822\n Loss [[0.30675343]] in iteration 823\n Loss [[0.30656978]] in iteration 824\n Loss [[0.30638641]] in iteration 825\n Loss [[0.30620333]] in iteration 826\n Loss [[0.30602053]] in iteration 827\n Loss [[0.30583802]] in iteration 828\n Loss [[0.30565579]] in iteration 829\n Loss [[0.30547383]] in iteration 830\n Loss [[0.30529216]] in iteration 831\n Loss [[0.30511077]] in iteration 832\n Loss [[0.30492966]] in iteration 833\n Loss [[0.30474883]] in iteration 834\n Loss [[0.30456828]] in iteration 835\n Loss [[0.304388]] in iteration 836\n Loss [[0.304208]] in iteration 837\n Loss [[0.30402828]] in iteration 838\n Loss [[0.30384884]] in iteration 839\n Loss [[0.30366967]] in iteration 840\n Loss [[0.30349077]] in iteration 841\n Loss [[0.30331215]] in iteration 842\n Loss [[0.3031338]] in iteration 843\n Loss [[0.30295573]] in iteration 844\n Loss [[0.30277792]] in iteration 845\n Loss [[0.30260039]] in iteration 846\n Loss [[0.30242313]] in iteration 847\n Loss [[0.30224615]] in iteration 848\n Loss [[0.30206943]] in iteration 849\n Loss [[0.30189298]] in iteration 850\n Loss [[0.3017168]] in iteration 851\n Loss [[0.30154088]] in iteration 852\n Loss [[0.30136524]] in iteration 853\n Loss [[0.30118986]] in iteration 854\n Loss [[0.30101475]] in iteration 855\n Loss [[0.3008399]] in iteration 856\n Loss [[0.30066532]] in iteration 857\n Loss [[0.30049101]] in iteration 858\n Loss [[0.30031695]] in iteration 859\n Loss [[0.30014316]] in iteration 860\n Loss [[0.29996964]] in iteration 861\n Loss [[0.29979638]] in iteration 862\n Loss [[0.29962338]] in iteration 863\n Loss [[0.29945064]] in iteration 864\n Loss [[0.29927816]] in iteration 865\n Loss [[0.29910594]] in iteration 866\n Loss [[0.29893398]] in iteration 867\n Loss [[0.29876228]] in iteration 868\n Loss [[0.29859083]] in iteration 869\n Loss [[0.29841965]] in iteration 870\n Loss [[0.29824872]] in iteration 871\n Loss [[0.29807805]] in iteration 872\n Loss [[0.29790764]] in iteration 873\n Loss [[0.29773748]] in iteration 874\n Loss [[0.29756758]] in iteration 875\n Loss [[0.29739793]] in iteration 876\n Loss [[0.29722853]] in iteration 877\n Loss [[0.29705939]] in iteration 878\n Loss [[0.2968905]] in iteration 879\n Loss [[0.29672187]] in iteration 880\n Loss [[0.29655348]] in iteration 881\n Loss [[0.29638535]] in iteration 882\n Loss [[0.29621747]] in iteration 883\n Loss [[0.29604983]] in iteration 884\n Loss [[0.29588245]] in iteration 885\n Loss [[0.29571532]] in iteration 886\n Loss [[0.29554843]] in iteration 887\n Loss [[0.2953818]] in iteration 888\n Loss [[0.29521541]] in iteration 889\n Loss [[0.29504926]] in iteration 890\n Loss [[0.29488337]] in iteration 891\n Loss [[0.29471772]] in iteration 892\n Loss [[0.29455231]] in iteration 893\n Loss [[0.29438715]] in iteration 894\n Loss [[0.29422224]] in iteration 895\n Loss [[0.29405756]] in iteration 896\n Loss [[0.29389314]] in iteration 897\n Loss [[0.29372895]] in iteration 898\n Loss [[0.29356501]] in iteration 899\n Loss [[0.29340131]] in iteration 900\n Loss [[0.29323785]] in iteration 901\n Loss [[0.29307463]] in iteration 902\n Loss [[0.29291165]] in iteration 903\n Loss [[0.29274891]] in iteration 904\n Loss [[0.29258641]] in iteration 905\n Loss [[0.29242414]] in iteration 906\n Loss [[0.29226212]] in iteration 907\n Loss [[0.29210033]] in iteration 908\n Loss [[0.29193878]] in iteration 909\n Loss [[0.29177747]] in iteration 910\n Loss [[0.2916164]] in iteration 911\n Loss [[0.29145556]] in iteration 912\n Loss [[0.29129495]] in iteration 913\n Loss [[0.29113458]] in iteration 914\n Loss [[0.29097444]] in iteration 915\n Loss [[0.29081454]] in iteration 916\n Loss [[0.29065487]] in iteration 917\n Loss [[0.29049543]] in iteration 918\n Loss [[0.29033623]] in iteration 919\n Loss [[0.29017726]] in iteration 920\n Loss [[0.29001851]] in iteration 921\n Loss [[0.28986]] in iteration 922\n Loss [[0.28970172]] in iteration 923\n Loss [[0.28954367]] in iteration 924\n Loss [[0.28938585]] in iteration 925\n Loss [[0.28922826]] in iteration 926\n Loss [[0.28907089]] in iteration 927\n Loss [[0.28891376]] in iteration 928\n Loss [[0.28875685]] in iteration 929\n Loss [[0.28860017]] in iteration 930\n Loss [[0.28844371]] in iteration 931\n Loss [[0.28828748]] in iteration 932\n Loss [[0.28813147]] in iteration 933\n Loss [[0.2879757]] in iteration 934\n Loss [[0.28782014]] in iteration 935\n Loss [[0.28766481]] in iteration 936\n Loss [[0.2875097]] in iteration 937\n Loss [[0.28735482]] in iteration 938\n Loss [[0.28720016]] in iteration 939\n Loss [[0.28704572]] in iteration 940\n Loss [[0.2868915]] in iteration 941\n Loss [[0.28673751]] in iteration 942\n Loss [[0.28658373]] in iteration 943\n Loss [[0.28643018]] in iteration 944\n Loss [[0.28627684]] in iteration 945\n Loss [[0.28612373]] in iteration 946\n Loss [[0.28597083]] in iteration 947\n Loss [[0.28581815]] in iteration 948\n Loss [[0.28566569]] in iteration 949\n Loss [[0.28551345]] in iteration 950\n Loss [[0.28536142]] in iteration 951\n Loss [[0.28520962]] in iteration 952\n Loss [[0.28505803]] in iteration 953\n Loss [[0.28490665]] in iteration 954\n Loss [[0.28475549]] in iteration 955\n Loss [[0.28460454]] in iteration 956\n Loss [[0.28445381]] in iteration 957\n Loss [[0.2843033]] in iteration 958\n Loss [[0.28415299]] in iteration 959\n Loss [[0.2840029]] in iteration 960\n Loss [[0.28385303]] in iteration 961\n Loss [[0.28370336]] in iteration 962\n Loss [[0.28355391]] in iteration 963\n Loss [[0.28340467]] in iteration 964\n Loss [[0.28325564]] in iteration 965\n Loss [[0.28310682]] in iteration 966\n Loss [[0.28295821]] in iteration 967\n Loss [[0.28280981]] in iteration 968\n Loss [[0.28266162]] in iteration 969\n Loss [[0.28251364]] in iteration 970\n Loss [[0.28236586]] in iteration 971\n Loss [[0.2822183]] in iteration 972\n Loss [[0.28207094]] in iteration 973\n Loss [[0.28192379]] in iteration 974\n Loss [[0.28177685]] in iteration 975\n","name":"stdout"},{"output_type":"stream","text":" Loss [[0.28163011]] in iteration 976\n Loss [[0.28148358]] in iteration 977\n Loss [[0.28133725]] in iteration 978\n Loss [[0.28119113]] in iteration 979\n Loss [[0.28104522]] in iteration 980\n Loss [[0.28089951]] in iteration 981\n Loss [[0.280754]] in iteration 982\n Loss [[0.2806087]] in iteration 983\n Loss [[0.28046359]] in iteration 984\n Loss [[0.2803187]] in iteration 985\n Loss [[0.280174]] in iteration 986\n Loss [[0.2800295]] in iteration 987\n Loss [[0.27988521]] in iteration 988\n Loss [[0.27974112]] in iteration 989\n Loss [[0.27959723]] in iteration 990\n Loss [[0.27945354]] in iteration 991\n Loss [[0.27931005]] in iteration 992\n Loss [[0.27916675]] in iteration 993\n Loss [[0.27902366]] in iteration 994\n Loss [[0.27888077]] in iteration 995\n Loss [[0.27873807]] in iteration 996\n Loss [[0.27859557]] in iteration 997\n Loss [[0.27845327]] in iteration 998\n Loss [[0.27831116]] in iteration 999\nFinal training loss  0.27831116480021495\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_tweet(tweet, freqs, theta):\n    \"\"\"\n    Input:\n    tweet: a string containg a tweet\n    freqs: frequency dictionary containing +ve,-ve frequencies of all words in the corpus\n    theta\" trained weights\n    Output:\n    pred: probability of input tweet being +ve or -ve\n    \"\"\"\n    x = extract_features(tweet, freqs)\n    pred = 1/(1+np.exp(-(np.dot(x,theta))))\n    return pred","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n    print(\" {}  {} \".format(tweet, predict_tweet(tweet, freqs,theta)))","execution_count":53,"outputs":[{"output_type":"stream","text":" I am happy  [[0.51613003]] \n I am bad  [[0.49453856]] \n this movie should have been great.  [[0.51251768]] \n great  [[0.51281724]] \n great great  [[0.52561764]] \n great great great  [[0.53838445]] \n great great great great  [[0.55110112]] \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check performance on test set\ny_hat = []\nfor tweet in X_test:\n    y_hat.append(predict_tweet(tweet, freqs,theta) >0.5)\naccuracy = (np.squeeze(y_hat) == np.squeeze(y_test)).sum()/len(X_test)\nprint(\"Test accuracy of twitter analysis is {}%: \".format(accuracy*100))","execution_count":54,"outputs":[{"output_type":"stream","text":"Test accuracy of twitter analysis is 99.5%: \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}