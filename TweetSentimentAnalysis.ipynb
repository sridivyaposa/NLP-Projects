{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import twitter_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Download Twitter samples\nnltk.download('twitter_samples')\nprint(\"Fields \",twitter_samples.fileids())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get positive and negative tweets from respective json files\npositive_tweets = twitter_samples.strings('positive_tweets.json')\nnegative_tweets = twitter_samples.strings('negative_tweets.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at sample tweets\npositive_tweets[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport string\nfrom nltk.tokenize import TweetTokenizer\n#Function for preprocessing the data and to get feature vectors\ndef process_tweet(tweet):\n    \"\"\"\n    Generate tokens from the given tweet\n    input: \n        tweet: A string containing a tweet\n    output: Tokens after\n        clean_tokens: Tokens of the processed tweet\n    \"\"\"\n    stop_words = stopwords.words('english')\n    stemmer = PorterStemmer()\n    #Remove hyperlinks, Retweet \"RT\" text, #, $stock tickers\n    tweet = re.sub(r'\\$\\w*','',tweet) # Removes words like $StockTicker #\\w --> word character\n    tweet = re.sub(r'RT\\s*','',tweet) # Remove RT text # \\s --> space character\n    tweet = re.sub(r'https?:\\/\\/\\S*','',tweet) # Removes links # ? --> 0 or 1 occurence of previous charcter #\\S any character except space characters (opposite of \\s)\n    tweet = re.sub(r'#','',tweet)\n    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True, preserve_case=False)\n    tokens = tokenizer.tokenize(tweet)\n    clean_tokens = []\n    for word in tokens:\n        if (word not in stop_words) and (word not in string.punctuation):\n            clean_tokens.append(stemmer.stem(word))\n    return clean_tokens\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function for preprocessing the data and to get feature vectors\ndef build_frequencies(tweets, sentiments):\n    \"\"\"\n    Build positive and negative frequencies of each word in the corpus\n    Input:\n        tweets: A list of tweets\n        sentiments: a list of corresponding sentiments\n    Output:\n        freqs: A dictionary containing frequencies --> (word,sentiment):frequency\n    \"\"\"\n    freqs = {}\n    for tweet,sentiment in zip(tweets,sentiments):\n        for token in process_tweet(tweet):\n            freqs[(token,sentiment)] = freqs.get((token,sentiment),0) + 1\n        \n    return freqs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combine positive and negative tweets and prepare train and test sets:\n#positive_tweets.extend(negative_tweets)\nall_tweets = positive_tweets + negative_tweets\nall_sentiments = np.ones(5000).tolist() + np.zeros(5000).tolist()\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(all_tweets,all_sentiments,test_size=0.2,random_state=101)\nprint(\"length of training tweets : \",len(X_train))\nprint(\"length of testing tweets : \",len(X_test))\nfor tweet,label in zip(X_train[:5],y_train[:5]):\n    print(\" {} {} \".format(tweet,label))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freqs = build_frequencies(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression:**\n$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n\nSigmoid\n$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$ \n\nCost Function\n$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) $$\n\nGradient of the Cost function:\n$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j $$\n\nUpdate the weights by subtracting fraction of derivative of loss function from weights:\n$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(x,y,theta,alpha, num_iters):\n    \"\"\"\n    Perform gradient descent of logistic regression\n    Input:\n        x: feature matrix(m,n+1), n+1 --> number of features including the bias\n        y: True label(m,1)\n        theta: Weights(n+1,1)\n        alpha: Learning rate\n        num_iters: Number of iteration you want to train your model for\n    Output:\n        J: Final loss after num_iters\n        theta: Adjusted weights after num_iters\n    \"\"\"\n    m = x.shape[0]\n    for i in range(num_iters):\n        z = np.dot(x,theta)\n        h = 1/(1+np.exp(-z))\n        J = -1./m * (np.dot(y.transpose(),np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))\n        theta = theta - (alpha/m) * np.dot(x.transpose(), (h - y))\n        print(\" Loss {} in iteration {}\".format(J, i))\n    J = float(J)\n    return J, theta\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_features(tweet,freqs):\n    \"\"\"\n    Extract features for each tweet using the freqs dictionary\n    Input: \n    tweet: A string containing a tweet\n    freqs: Frequency dictionary containing frequencies for each word -->(word,label):freq\n    Output:\n    x: feature vector(1,3)\n    \"\"\"\n    x = np.zeros((1,3))\n    x[0,0] = 1 #bias\n    for word in process_tweet(tweet):\n        x[0,1] += freqs.get((word,1),0)\n        x[0,2] += freqs.get((word,0),0)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract features from each tweet and append to feature matrix X\nm = len(X_train)\nX = np.zeros((m,3))\nfor i in range(m):\n    X[i,:] = extract_features(X_train[i],freqs)\nY = np.array(y_train)\nY = Y.reshape(-1,1)\nprint(\"Y shape: \",Y.shape)\n#Gradient Descent\nJ, theta = gradient_descent(X,Y,np.zeros((3,1)),1e-9,1000)\nprint(\"Final training loss \",J)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_tweet(tweet, freqs, theta):\n    \"\"\"\n    Input:\n    tweet: a string containg a tweet\n    freqs: frequency dictionary containing +ve,-ve frequencies of all words in the corpus\n    theta\" trained weights\n    Output:\n    pred: probability of input tweet being +ve or -ve\n    \"\"\"\n    x = extract_features(tweet, freqs)\n    pred = 1/(1+np.exp(-(np.dot(x,theta))))\n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n    print(\" {}  {} \".format(tweet, predict_tweet(tweet, freqs,theta)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check performance on test set\ny_hat = []\nfor tweet in X_test:\n    y_hat.append(predict_tweet(tweet, freqs,theta) >0.5)\naccuracy = (np.squeeze(y_hat) == np.squeeze(y_test)).sum()/len(X_test)\nprint(\"Test accuracy of twitter analysis is {}%: \".format(accuracy*100))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}